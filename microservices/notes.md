- Monolith vs Microservice
  - A monolith contains all the routing, middlewares, business logic, and database access required to implement _all features_ of an app
  - A single microservice contains all of that for _one feature_ of an app. Everything is self contained.
- The big problem of microservices is _data management between services_, due to how data is stored and accessed
  - Each service gets its own db (_Database-per-Service_), and services _never_ reach into the db of another service
    - each service should be independent of other services
    - db schema/structure might change
    - some services might function more efficiently with different types of DB's (sql vs nosql)
- communication strategies between services (not the same meaning as JS!)
  - **sync**: services communicate with each other using _direct requests_
    - conceptually easy to implement
    - could circumnavigate the need for a db
      - if service C depends on data from services A & B, then C doesn't need a db
    - introduces dependency between services
    - if any inter-service request fails, the overall request fails
    - the entire request is only as fast as the slowest request
    - can introduce complicated webs of requests
      - maybe services A & B also make requests to other services
  - **async** (2 ways): services communicate eith each other using _events_ in the _event bus_:
    - event broker: receives events, sends them to interested parties
    - each service emits events to the bus _when they need data_, other services process and return; like a reducer for all services
      - shares all the downsides of sync, not very popular in the wild
    - services emit an event to event bus _whenever they make entries into their own db_, other services listening for that event type then add the entry to their respective db
      - zero depedencies amongst services
      - very fast, no need to navigate a web of requests
      - data duplication; paying for extra storage (haha), extra db
      - more complicated
- Cors requests (cross origin request sharing)
  - common error with microservices
  - mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served
  - browser will prevent request unless receiving server provides a specific set of headers
    - wire up the cors npm package in server as a middleware with `app.use(cors());`
- API request minimization strategies
  - react example: get all posts and their associated comments
  - sync
    - GET request to posts service, post service requests all comments from comments service, post service assembles and returns
  - async
    - posts/comments services emit an event to event bus any time a post/comment is created
    - query service assembles all posts/comments into an efficient data structure
      - GET request goes to query service instead of posts
- Event Bus
  - several implementations: RabbitMQ, Kafka, NATS
  - receives events, publishes them to listeners
  - many different subtle features that make async communication easier or harder
  - in Express, event is contained in the `req.body` property
- Moderation (whether to approve or reject an action)
  - extract moderation logic into its own service, not in react app (would require frequent redeploys)
  - react app only needs to tell diff between pending, rejected and approved moderation
  - Option #1: middleman approach: introduce moderation service, communicates event creation to query service
    - comments service emits event saying comment was created
      - moderation service picks up event, processes comment, emits new event saying it was moderated
        - query service persists comment
    - can creates delays between user submitting comment and it being persisted by query service, especially if moderation is approved by hoomans
      - the longer the wait, the worse the UX
    - introduces irrelevent logic and overcomplication to query service as app grows in complexity
  - Option #2: Moderation updates status at both comments and query services
    - comments service emits event saying comment was created
      - both moderation and query service pick up event
        - query service persists immediately with default status of 'pending'
          - moderation service eventually processes it, emits new event saying it was moderated
            - query service updates status
    - solves problem of delay (can display placeholder comment for UX)
    - introduces irrelevent logic and overcomplication to query service as app grows in complexity
      - query service is for presentation logic (storing posts and their comments together to avoid multiple API requests)
      - maybe comment can be up/downvoted, promoted, searchable, advertised, etc., query service doesn't/shouldn't care!
        - that business logic should live in the comment service
  - Option #3: query service only listens for 'update' events
    - user submits comment, comment service persists event _with status prop_ (of pending) and emits event
      - moderation and query services pick up the creation event
        - moderation processes specialized update and emits event _back to comment service_
        - query service persists comment immediately (with pending status) for UX placeholder
          - comment service updates status and emits update event
            - query service picks up update event, updates comment
    - splits events into specialized/specific event (CommentModerated) and general event (CommentUpdated)
    - most modular approach: keeps comment business logic in the comment service
- dealing with missing requests
  - if a service goes down and an event missed out on being processed, like a deadletter queue
  - if a new service was introduced, and never had the chance to process events
  - Option #1: Sync Requests
    - new service makes a direct network request to other services for all their db items
    - not great: requires new code in other services to handle these requests
  - Option #2: Direct DB access
    - give new service direct access to the other services db's, bypass the services
    - new service could make its own queries, but that requires potentially a lot of extra code
      - maybe other services have different db types (postgres, mysql, mongo, dynamo, etc)
  - Option #3: Store Events
    - whenever any service emits any event to the event bus, bus stores each event in its own db
    - a new service can request all events from the bus, and process them without any extra code
    - if a service goes down, look at the last event received and request any that happened after to fill in the gap
    - only downside is storage, but data storage is so cheap now that it doesn't matter
- Deployment
  - to deploy online we could rent a virtual machine from DigitalOcean, AWS, Azure, etc., and run the localhost setup there
    - if one service gets overloaded, we could create duplicate instances on the same VM, behind a load balancer
      - these copies would have to be allocated additional ports, which would have to be hardcoded in the event bus
        - ties the number of services we're running with the code implementation => _lame_
    - instantiating duplicate services on a second VM would be even more tedious
      - still have to hardcode the ports _and_ configure ip routing
    - spinning up duplicate services for peak traffic hours would require hardcoding in the event bus too => _gross_
  - need to:
    - keep track of all the different services running
    - have ability to create new copies of services on the fly
    - automatically figure out if a service is running
    - whether to make contact between two services
      - enter **docker** and **kubernetes**

## Docker

- containers: isolated computing environment, contains everything needed to run a program
  - specific env setup and knowledge of how to start it; configured in container
  - use a container for each service, and spin up duplicates easily
  - 1:1 pairing to docker containers and each instance of the running program
  - kernel isolates a section of necessary processes/resources (RAM, network, CPU, harddrive, etc) for container
  - running process with a subset of physical resources that are specifically allocated to that process
    - isolated by _namespacing_
    - can limit amount of resources used per process with _control groups_ (cgroups)
      - namespacing and cgroups are specific to linux
        - docker installs a Linux VM, inside which the containers are created
          - linux kernal hosts all the running processes in the containers
- image
  - file system snapshot
  - startup command
- CLI
  - `docker run <image name>` = `docker create` + `docker start`
    - include override (to ignore default startup command) with `docker run <image name> command`
      - e.g. `docker run busybox ls` => lists out the file system structure within the image
      - override command must exist within the image
    - `docker create` preps the container file system
    - `docker start -a <container id>`
      - `-a` ('attach'?) makes docker watch for output from container and print to terminal
      - by default, `run` will show all the logs from the container, `start` will not
  - `docker ps`
    - list all running containers
    - modify command to show all containers that have ever been created on machine with `--all`
    - useful to get the id of a running container
  - `docker logs <container id>` to look at container and retrieve all info that has been emitted
  - `docker stop <container id>` to stop a container
    - hardware signal SIGTERM (terminate signal) is sent to primary process in that container to shut itself down
      - gives time to save files or emit signals, etc
    - if container doesn't automatically stop within 10 seconds, docker automatically issues the kill command
  - `docker kill <container id>` to kill a container
    - sends SIGKILL command to immediately shut down with no additional work
      - useful if container froze, unresponsive
  - `docker system prune`
    - delete stopped container, all networks not used by at least one container, all dangling images, all build cache
  - multi-command containers
    - if we need to execute second command inside a running container, e.g. `redis-cli` to interact with redis instance running in container
    - basically grants shell access to the container
    - `docker exec -it <container id> <command>`
      - `exec`: run another command
      - `-it`: (interactive?) allow us to provide input to the container, actually 2 commands:
        - `-i`: attach terminal to STDIN (standard in) channel of that process
        - `-t`: basically formats text with STDOUT
          - STDIN, STDOUT, STDERR from Linux kernal
      - `docker exec -it <container id> sh`
        - `sh` is actually a program, executed in that container
          - command processor aka a shell, like zsh or bash
          - gives full terminal access inside the container, can run typical commands from the unix env
          - most containers have `sh` program already included
  - `docker build <build context>` takes a Dockerfile and generates an image out of it
    - tied to the Docker-CLI, for creating docker images
    - context is the set of files and folders we want to encapsulate in the container
    - _tagging an image_
      - `docker build -t <your docker id>/<repo or project name>:<version> <directory to use for build>`
        - `docker build -t myDockerId/redis:latest .`
  - `docker commit` for manual image generation, but Dockerfile and build is more common
- container lifecycle
  - when you start a container that's already been created, cannot replace default command
    - container will alway use original command from when container was first created
- creating docker images
  - create a Dockerfile, containing configuration to define container behavior
  - pass file off to Docker Client (docker cli), which passes it to Docker Server, which builds usable image
  - flow for creating a Dockerfile
    - specify base image
    - add config to run some commands to install additional programs/dependencies
    - specify a command to run on container startup
  - uses a cache behind the scenes to improve rebuild speed
  - common commands in Dockerfile
    - FROM: specify base image
    - WORKDIR: set working directory in container; all commands will be issued relative to the directory
    - COPY: copy over a file
    - RUN: run a command
    - CMD: set command to run when the container starts up

## Kubernetes (aka K8s)

- tool for running a bunch of containers in a _cluster_
  - a cluster contains any number of VM's, each referred to as a node
    - nodes managed by a _Master_, a program that manages everything in the cluster
      - give it some config files to describe how we want our containers to run and interact with each other
        - send requests to cluster, cluster automatically figures out how to route requests to appropriate service
        - makes communication easy, e.g. between event bus node and service nodes
        - makes scaling services easy
      - kube looks locally first for images in config file, if not found, looks on Docker Hub
      - creates containers with image, distributes the containers amongst nodes
      - containers are hosted in a _pod_, inside the node
      - to manage pods, kubectl creates a _deployment_
        - reads config file for instructions
        - automatically recreates pods if they fail
      - kube creates a _service_ to manage node-to-node communication
        - not a running program or server, gives access to running pods (or containers in the pods) inside cluster
        - abstracts difficulty in handling networking amongst IP port routing
        - great for microservice event bus; pod will reach out to _service_ in order to contact another pod
- kubernetes cluster
  - a collection of nodes + a master to manage them
- node
  - a virtual machine that runs containers
- pod
  - more or less a running container; technically a pod can run multiple containers
  - if you create a pod without a version number, docker tags it as _latest_
    - k will default search k hub for image if there's no version number; if not found, error!
- deployment
  - monitor/manage a set of pods, make sure they are running and restarts them if they crash
  - useful for versioning, takes care of updates automatically
  - `apiVersion: apps/v1` => deployment lives in apps bucket
  - `replicas: 1` => number of pods we want to create, running some image
  - `selector` tells deployment which pods its supposed to manage
  - `template` where we specify exact config of a pod we want deployment to create
- service
  - provides an easy-to-remember URL to access a running container
- config files
  - tells kubernetes about the different Deployments, Pods and Services (referred to as "Objects") that we want to create
  - written in YAML
  - always store these files with project source code - free documentation!
  - do not create Objects _without_ config files
    - you can run direct commands to create Objects on command line, only do this for testing
- basic commands
  - `kubectl get pods`
    - print out info about all of the running pods
  - `kubectl exec -it [pod_name] [cmd]`
    - soon to be deprecated; use `kubectl exec [POD] -- [COMMAND]` instead
    - execut the given command in a running pod
  - `kubectl logs [pod_name`
    - print out logs from the given pod
  - `kubectl delete pod [pod_name]`
    - deletes the given pod
  - `kubectl apply -f [config file name]`
    - tells k to process the config
    - apply multiple config files with `kubectl apply -f .` for all in directory
  - `kubectl describe pod [pod_name]`
    - print out some info about the running pod
- deployment commands
  - `kubectl get deployments`
    - list all running deployments
  - `kubectl describe deployment [depl_name]`
    - print out details about specific deployment
  - `kubectl apply -f [config file name]`
    - create a deployment out of a config file, 'apply' the config to a cluster
  - `kubectl delete deployment [depl_name]`
    - delete a deployment
- updating deployments
  - method #1: not used often professionally
    - make change to project code
    - rebuild image, specifying new image version
    - in deploy config file, update image version
      - introduces chance of error by having to manually update
    - run command `kubectl apply -f [depl_file_name]`
  - method #2: preferred process for dev env
    - deployment must be using _latest_ tage in the pod spec section
      - k8s will auto update deployment version if latest is updated
    - make an update to your code
    - build the image
    - push the image to docker hub
    - run command `kubectl rollout restart deployment [depl_name]`
  - or just use _Skaffold_ to automate method 2 (see below)
    - developed by Google Cloud team
    - automates many tasks in k8s dev env
    - easy updates for code in a _running pod_
    - easy to create/delete all objects tied to a project
- Services
  - another _object_ like a pod or deployment, also created by config file
  - used to set up communication between different pods, or access from outside the cluster
  - types of services
    - Cluster IP
      - sets up an easy-to-remember URL to access a pod, only exposes pods _in the cluster_
      - network different pods together
      - make a request to url with `http://<service_name>/<port>/someRoute`
    - Node Port
      - makes a pod accessible from _outside the cluster_, usually used for dev purposes
    - Load Balancer
      - makes a pod accessible from _outside the cluster_, this is the right way to expose a pod to the outside world
    - External Name
      - redirects an in-cluster request to a CNAME url
  - _spec selector_, choose which pods to expose, matches pods by their _label_ like css class names
  - port vs targetPort
    - port attached to the container, e.g. express app.listen(4000), is the _targetPort_
    - port attached to the service, which interacts with browser, is the _port_
    - usually just keep them the same
  - nodePort => randomly assigned port between 30000-32767
    - access from outside world to the node service, for dev purposes
    - `localhost:3xxxx/<pod name>`
- to add multiple objects to a single yaml file, use `---` for linebreak
- `type: ClusterIP` is optional as kube will default to clusterip

Integrating React App into K Cluster with load balancer service

- React application is actually Create-React-App Dev server
- house it in a container, in a pod
  - create image of app and deploy to docker hub
  - requires a deployment config to host inside cluster
    - and a clusterIP so ingress nginx controller can direct traffic to pod
- dev server initially serves up HTML/CSS/JS to the browser, and becomes irrelevant after initial request
- After initiating, all requests for data from browser need to go to the components inside the k cluster pods
  - for browser to reach out to pods, we have 2 options
- option #1: no good!
  - create a NodePort service for each pod, to expose it to the outside world
    - Node Ports are creating with quasi random ports
    - changing node ports would require updating ports in the react app code
- option #2:

  - **load balancer service**
    - one single point of entry to the cluster, which will route requests to appropriate cluster ip service
    - tells k8s to reach out to its cloud provider (aws, gc, azure) and provision a load balancer, gets traffic in to a _single pod_
      - load balancer exists outside our cluster, in the provider env
        - directs outside traffic into pods
  - **ingress or ingress controller**
    - technically 2 different things but basically:
      - a pod with a set of routing rules to distribute traffic to other services
      - request comes into load balancer, which sends request to ingress controller
      - based on request's path and ingress routing rules, sends request to correct pod
        - (technically sends it to cluster ip service which then sends req to pod)
    - [ingress-nginx](https://github.com/kubernetes/ingress-nginx) (different from _kubernetes ingress_)
      - acts as the ingress controller
      - requires a config file containing router rules
      - ingress controller scans all the objects in our cluster for one with `annotations` metadata prop containing `kubernetes.io/ingress.class: "nginx"`
        - this signifies it contains all the routing rules
      - [concerning local development](https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/):
        - yaml line `- host: <domain.com>` require a tweak to Host File `/etc/hosts`
          - need sudo access, add domain to k8s ip, e.g. `127.0.0.1 <domain_name>.com`
          - tricks nginx to think we're coming to it from a specific domain instead of localhost
          - may have a "connection not private" error
            - nginx by default tries to use https with a self-sign certificate
            - not trusted by chrome: "Kubernetes Ingress Controller Fake Certificate"
            - type 'thisisunsafe' anywhere in browser window to bypass chrome security
              - only do this in a development situation!
      - gotchas
        - cannot differentiate HTTP request methods for routing
          - e.g. POST and GET requests to the same route of /posts
          - requires fix to code, rebuild code, update deployment manually => gross
        - cannot interpret wildcards in routes
          - `/posts/:id/comments` needs regex for :id
            - `/posts/?(.*)/comments`
            - requires addition to annotations
        - for SPA react app using router, default path in `paths` (last path) use wildcard
          - matches any path to always show react app, like `/` in react router

- [skaffold.dev](https://skaffold.dev/)
  - config runs outside of our cluster, not consumed by k8s
  - anytime a change is made in targeted _manifests_, skaffold reapplies it to cluster
    - also applies all deployments when skaffold starts
    - also deletes all related objects when skaffold is stopped
  - by default, whenever an image is rebuilt, it will try to push it to docker hub
  - `artifacts` designates something in project that needs to be maintained (`context`)
    - whenever something changes in that 'synced' directory (`src`), it will attempt to update pod directly (`dest`)
      - doesn't rebuild image, just takes changed file and inserts it into corresponding pod
    - if updated file does not match synced source, skaffold tries to rebuild the entire image and update deployment
      - e.g. adding new dependency, changing package.json
        - basically update in place or rebuild entire image
  - `skaffold dev`, the only command you need!
    - automatically outputs logs to terminal
  - NB: need automated restarting/change detections in app
    - even if skaffold updates a file in the pod, it won't restart the primary process of the pod
      - need something inside pod that will automatically restart the process
        - anytime create-react-app sees a file change, rebuilds react app and refreshes browser
        - nodemon watches the server files and restarts on change as well
  - sometimes has challenges detecting file changes inside containers
- skaffold on Google Cloud VM

  - changing a 'synced' vs 'unsynced' file regarding Google Cloud deployment
    - unsynced changes are detected and sent to 'google cloud build' service
      - rebuilds docker build for us with updated source code and Dockerfile
        - sends updated image to Google Cloud VM to update deployment
  - steps to connect to cloud cluster with _kubectl contexts_ i.e. connection settings
    - lists out auth credentials, users, ip addresses, etc, to tell kubectl how to connect to different clusters
    - create a new context config on the GC dashboard or install/use the [google-cloud-sdk](https://cloud.google.com/sdk/docs/quickstart)
      - login to gcloud sdk with `gcloud auth login`
        - `gcloud init` to configure k8s project
        - select corresponding account, project and region used to create k8s cluster on GCP
      - fetch context info to connect to GCP cluster
        - with docker running locally
          - run `gcloud container clusters get-credentials <cluster_name>`
          - without local docker
            - close Docker Desktop
            - run `gcloud components install kubectl`
            - run `gcloud container clusters get-credentials <cluster_name>`
          - see cluster context options by checking docker dropdown and hovering over _kubernetes_
      - enable google cloud build
        - gcp console left burger drawer, click 'Cloud Build' under tools section
          - enable cloud build api
      - update skaffold.yaml
        - update `build` to use `googleCloudBuild` with `projectId` instead of `local`
          - can only have one active at any one time (gcp build or local build)
        - update image name: `us.grc.io/<projectId>/<projectDirName>`
          - projectDirName comes from artifacts => context
          - update relevant deployments image names too
      - setup ingress-nginx on the gcloud cluster (gce-gke)
        - [kubernetes.github.io/ingress-ngingx](https://kubernetes.github.io/ingress-nginx/deploy/#gce-gke)
          - sets up _ingress controller_ and _load balancer_
          - confirm k8s context is set to gck before running command
      - update /etc/hosts file to point to remote cluster
        - redirect traffic to gce load balancer instead of localhost
        - get LB IP from GCP dashboard => networking => network services => load balancing
      - restart skaffold with `skaffold dev`
        - it applies deployment config file and routing rules to gCloud cluster
        - output logs are from remote container!
      - NB: had to reauthenticate with gcloud for some reason
        - `gcloud auth application-default login`
        - see [stackoverflow post](https://stackoverflow.com/questions/41507904/could-not-find-default-credentials)
        - to see gcloud build history => check gcp dashboard menu => tools => cloud build => history
  - `kubectl config --help` to find commands to configure cluster/context info

- [mongo](https://hub.docker.com/_/mongo) containerized db
  - default port 27017
  - all data is lost whenever deleting or restarting the pod by default
  - connect with **mongoose**
    - getting TS and Mongoose (m) to cooperate
      - m type definition file doesn't communicate correct properties easily to TS
      - m adds some properties not explicitly defined in the document/constructor
        - e.g. createdAt, updatedAt, timestamps, db stuff etc
        - workaround: tell TS that there's two different sets of properties with interfaces
          - one set of props passed to the constructor, a second set accessible on the document it creates
          - option 1: custom function that associates interface with `new User` call to ensure correct arguments
            - the function can then be exported and used in lieu of new User
          - option 2: add custom function with types to the static properties of the model
            - `schema.statics.methodName`
            - accomplishes the same thing but adds the type checks to the Model, removes superfluous exports
            - need to tell TS about the static function added to the model with another interface
          - basically, `statics` is how we add a new method to the model
            - to add a new method to the _document_, use `methods`
              - `schema.methods.myMethod = function() {}`
      - NB
        - m user model represents entire collection of models
        - m user document represents one single user
        - m types are capital (`String`); TS types are lowercase (`string`)
    - salting the hash
      - static methods can be accessed without creating an instance of the class
        - can call `Password.toHash` without having to call `new Password`
      - `scrypt` hashing function that is callback based
        - to use in conjunction with async/await, convert to promises with `promisify`
        - scrypt returns a Buffer
          - similar to an array of integers but corresponds to a raw memory allocation outside the V8 heap
      - mongoose pre-save hooks
        - when saving user to db, intercept the save attempt with mongoose middleware function
          - take the password set on the user document, hash it, overwrite the password on the document
            - `userSchema.pre('save', async function(done) { do stuff... done() })`
              - mongoose support for async/await isn't great, so we have to manually call `done`
              - uses `function` instead of fat arrow
                - with middleware function, we have access to the document being saved as `this`
                - fat arrow function binds the context of `this` to the enveloping scope which we don't want
  - associate two different documents/records together, 2 primary strategies
    - option #1 - embedding
      - eg embed the ticket data in the order
      - querying is a bit challenging
      - where do we put an unreserved ticket?
    - option #2 - mongoose ref/population feature
      - create a _collection_ of each document
        - each new doc can optionally have a reference to the other collection
- Authentication Strategies: _are they logged in?_
  - user auth with microservices is an unsolved problem, there are many ways to do it, no one right way
  - Option **#1**
    - individual services rely on the auth service
      - e.g. purchasing service sends [JSON web token](https://jwt.io/) (JWT), cookie, etc, via _sync request_ to auth service
        - if auth service goes down, any other service relying on auth will automatically fail
        - changes to auth state are immediately reflected
        - reminder: sync = direct request from one service to another, without event bus; not JS
  - Option **#1.1**
    - individual services rely on the auth service as a gateway
      - auth contains logic to inspect JWT/cookie and decide if user is authenticated
        - blocks or forwards request along to intended destination, e.g. purchasing service
        - same pros and cons of option 1
  - Option **#2**
    - individual services know how to authenticate a user
      - no dependency on a gateway or outside service
      - self contained service, doesn't care about an auth service
      - some down sides:
        - creates duplicate authentication code amongst services
          - not too bad, can extract into a shared library
        - stale cookies/tokens
          - changes to auth state are not immediately reflected
            - after authentication, auth checks go against the cookie, not auth service
            - any update to account state will not be seen until account logs out and logs back in
          - workaround strategies
            - set an expiration date on the cookie (e.g. 15 secs, mins, or hours)
            - if there's no room for error (malicious things could happen before expiration)
              - emit events on the bus with a short-lived cache to reflect auth state
                - cache persists as long as the expiration timer, as there's no need after expiration
  - cookies != JSON web tokens
    - cookies are a transport mechanism
      - can move any kind of data between browser and server
      - server response can include a Header of `Set-Cookie` with any value
      - browser appends that cookie to headers of any followup requests to the same domain/port
      - stored in/managed by browser, automatically included in followup requests
    - JWT's are an authentication/authorization mechanism
      - stores any data we want, have to manage it manually
      - payloads are stored in a token; to communicate it from browser to server
      - can include **in 'Authorization' header** with jwt inside it
      - can include token **in the body** of the (post, put, delete) request
      - can also mix and match and store JWT **inside a cookie**
  - requirements for option 2 auth mechanism
    - **must be able to:**
      - tell us user details
      - handle authorization info
      - have a built-in, tamper-resistant way to expire/invalidate itself
      - be easily understood by many languages/different backends
      - must not require a data store on the service to handle auth request
    - well suited for JWT
      - has its own self destruct mechanisms
      - cookie expiration is handled by the browser
        - cookies tend to require the need for a data store, e.g. sessionId
        - can be copied and reused down the line
  - Auth Service
    - steps for sign in
      - does a user with this email exist? if not, respond with error
      - compare passwords of stored user to supplied pw
      - if the same, success
      - user now considered to be logged in, send JWT in cookie
    - current user
      - does user have a 'req.session.jwt'?
        - if not set, or invalid jwt, return null
        - if yes, return jwt payload
  - more auth middlewares
    - cookie headers on incoming request to any service will need:
      - middleware to extract jwt payload and set on `req.currentUser`
- **server-side rendering** (with NextJS)
  - useful for SEO search engine optimization and page load speed
    - user sees content appear on the screen much more quickly
      - particularly on a mobile device
  - review of loading process of a _typical_ react app into the browser
    - browser makes GET request, client responds with an HTML file
    - browser loads scripts and makes followup requests to get and execute those too
    - now loaded, browser requests privvy data, representing the initial need for auth
      - header, body or cookie would work
        - not with server-side bruh!
  - the main idea
    - is for the backend server (client) to respond to the initial GET request with display content
    - client builds fully rendered html file, with content, as response
      - circumvents need for browser to load scripts and make follow-up requests
      - client (NextJS) does that lifting, makes necessary followup requests
        - however, requires auth info with that first request
          - it's impossible to customize the initial request coming from the browser
          - eliminates use case for header and body type requests, leaving only the cookie (or jwt stored in a cookie)
            - or service workers... but thats a different story entirely
  - auth
    - in order to satisfy 'no data store' requirement while using a cookie, use [cookie-session](https://www.npmjs.com/package/cookie-session)
      - cookies can be challenging to handle across diff languages due to encryption (or really decryption on different backends)
        - JWTs, when used correctly, are naturally resistant to tampering, which allows one to skip encrypting the cookie
          - [jsonwebtoken](https://www.npmjs.com/package/jsonwebtoken) for generating a web token
            - [https://www.base64decode.org/](https://www.base64decode.org/) for decoding encrypted jwt
          - create a web token with `jwt.sign()`
          - `jwt.verify()` to verify token was not tampered and to pull data out of jwt payload
            - first arg is actual token, second arg is jwt key
            - wrap in try catch
            - (don't forget @types for cookie-session & jsonwebtoken)
          - although anyone can see the jwt payload, by verifying the signature with the _signing key_, we can ensure the payload has not been altered
            - need to secretly share the signing key with all consuming services with docker/kubernetes
      - cookie-session turns session object into a string,
        - cookie-session middleware then sends cookie back to users browser inside response `set-cookie` header
    - k8s secrets
      - create `Secret` type object, load into each container hosting consuming services as an environment variable
        - add secret to respective deployment config files
          - add `env` array to container
          - if referencing a secret inside a pod that doesn't exist, get a `CreateContainerConfigError`
            - to debug, run a `describe` on the pod
        - reference env var secret in the code in the container
          - to access env variable with Node JS, `process.env.<variable>`
          - TS never assumes an environment variable is defined
            - could check in file or (preferably) when app starts
      - imperative command: `kubectl create secret generic <name-of-secret> --from-literal=<key>=<value>`
        - _generic_ is a type of secret
        - use imperative command if you want to avoid listing secret in a declarative config file
          - have to remember the secrets yourself! best for dev or staging environments
- (re)formatting JSON properties
  - remap or delete properties of api responses to create consistent formatting when dealing with various sources
    - in _js console_, include `toJSON() { ... }` function and block in object
      - block logic will override any function calling the object, e.g. stringify, jsonify, etc
    - in _mongoose_, to apply to user document, add second argument, `{ toJSON: {} }`, to model Schema
      - `DocumentToObjectOptions` object with properties, unlike JS function
        - make direct changes to the `ret` @param (aka returned object) on the `transform` prop
      - writing view level logic into the model... oh well
- NextJS client directory - SSR
  - `npm install react react-dom next axios`
  - route inside a nextJS project via `pages` directory
    - nextjs interprets filenames in the _pages_ directory as distinct routes within the app
    - uses `index.js` as root route
  - to startup a next project, add script to run `next` in package.json
  - to run inside kubernetes cluster, build image Dockerfile and deployment
    - for local cluster, tag and push image
    - add entries to skaffold and ingress service
  - nextjs sometimes finicky with file change detection when running in docker container
    - setup next.config.js file for webpackDevMiddleware
      - `config.watchOptions.poll = 300`, webpack polls everything every 300 ms
    - next doesn't automatically restart itself when changes are made to this config file
      - restart (kill) pod manually to update nextjs config file
  - [global css](https://github.com/vercel/next.js/blob/master/errors/css-global.md) with bootstrap
  - layers between browser and auth service
    - ingress nginx load balancer => clusterIP service => pod running auth container => express app => route handler
  - forcible redirects with `Router` from `'next/router';`
    - add onSuccess callback for redirect into custom hook
  - what happens behind the scenes with incoming requests to NextJS client app
    - inspect URL of incoming request, determine set of components to show
    - call component's `getInitialProps` static method
      - only location where we can fetch data needed by components during SSR process
        - while NextJS attempts to render app on server
      - make request here to confirm authentication during SSR
        - can't use hooks as `getInitialProps` is not a component
          - it is a function executed on server side, not in browser
      - calling `getInitialProps` from within 2 different components introduces complexity
        - see 'pass along cookie' section below
      - NB: cannot do data loading inside of components
    - render each component with data from 'getInitialProps' _one time_
      - provided as a prop to component
    - assemble HTML from all components, send back response
  - `Error: connect ECONNREFUSED 127.0.0.1:80`
    - /etc/hosts file points ticketing.dev to ip (127.0.0.1), default port 80
      - this port/ip is bound to ingress nginx, which routes to client application (nextJs)
    - effects of making requests _without specifying the domain_
      - from within the _browser_, i.e. within a react component
        - browser assumes and prepends current domain to request, e.g. 127.0.0.1:80/request/path
        - k8s ingress nginx catches request, inspects path, routes to service, all good
          - network tab => request => headers => general => request URL
      - from server-side, i.e. within `getInitialProps`, within NextJS client
        - request defaults to Node http layer
        - Node assumes you're trying to make a request on local machine, assigns localhost (127.0.0.1:80)
          - problem is, NextJS client is being run from _within a k8s container itself_
            - request sent to localhost:80 within the container
            - doesn't get redirected to ingress-nginx or directly to target service
    - _solution_: configure how axios makes requests depending on where request is made
      - from browser
        - baseURL of "" empty string, allow browser to continue making the correct domain assumptions
      - from nextjs app during SSR phase
        - option 1: not great
          - attach k8s service domain to request
            - implies react client must know names and corresponding routes for everything, which we already configured with ingress-nginx
            - e.g. `http://auth-srv/api/users/currentuser`
        - option 2: better!
          - configure nextjs to send requests to ingress-nginx whenever it needs to query data from a k8s service
          - ingress-nginx service configured to know where to route requests based on just the path
            - how to make request directly to nginx from inside the cluster (in nextjs)?
            - how to manage cookie?
            - to make request from NextJS, need cookie from original incoming request, extract/include in request to nginx
          - only works when trying to access a different service inside the same **namespace**
            - k8s: all the different objects are created under a specific namespace, like a sandbox
            - `kubectl get namespace`
          - to do **Cross Namespace Service Communication** to a service in a different namespace
            - `kubectl get services -n ingress-nginx` to get services from another namespace
            - api request route param: `http://ServiceName.Namespace.svc.cluster.local`
              - e.g. `http://ingress-nginx.ingress-nginx.svc.cluster.local/api/request/path`
              - however, without a domain (host), k8s doesn't know what set of rules to use within ingress-srv.yaml
                - provide second argument, an object, to `axios.get()` api request
                  - `{ headers: { Host: 'targetHost.dev' } }`
                  - or use host from cookie extraction method below
              - to create simplified request to `http://ingress-nginx/srv`, create **external name service**
                - remaps domain of a request, like an alias, to look like option 1
          - pass along cookie
            - whenever `getInitialProps` is called on the nextjs server, it is automatically passed the `context` object as its first argument
              - context contains the `req` request object
                - check req.headers to get the cookie, and other stuff like the host
            - NB: arguments provided to `getInitialProps` function for a _page_ are different to those provided to a custom `_app` component
              - inside **page**, first arg = context === { req, res }
              - inside **custom app component**, req object is nested: context === { Component, ctx: { req, res }}
                - `console.log(Object.keys(appContext))` => [ 'AppTree', 'Component', 'router', 'ctx' ]
                - when tying gIP to \_app component, other gIP func's on individual _pages_ do not get automatically invoked
                  - appContext.Component is a reference to the _page_ being rendered
                    - manually call page .getInitialProps with AppContext.Component.getInitialProps
                    - manually split page props from app props in \_app wrapper
      - how do you know if code will be executed by browser or server (nextjs)
        - request from a component, always issued from browser
        - request from `getInitialProps` might be executed from the client or server
          - check: [if (typeof window === 'undefined')](https://github.com/vercel/next.js/issues/5354#issuecomment-520305040)
            - then you're on the server, make request to nginx,
          - server
            - hard refresh
            - clicking link from different domain
            - typing URL into address bar
          - client
            - navigating from one page to another while in the app
  - `Link`, custom component developed by NextJS
    - doesn't create anchor tag for you... create your own!
    - trick for conditionally showing links in header.js
      - make array of objects or null, filter out null, map objects into react html

Typescript

- javascript + a type system
- helps catch errors during development
- uses _type annotations_ to analyze code
  - type: easy way to refer to the different props + functions a value has
  - Primitive Types
    - number
    - string
    - boolean
    - symbol
    - void
      - not returning anything, can technically return null and undefined too
    - null
    - undefined
  - Object Types
    - functions
    - arrays
    - classes
    - objects
  - _any_ type
  - _never_ type: we will never reach the end of the function, e.g. throwing errors
  - type annotations: we tell TS what type of value a variable will refer to
    - when a function returns the _any_ type and we need to clarify the value
    - when we declare a var on one line and initialize it later
    - when we want a var to have a type that cannot be inferred
  - type inference: TS tries to figure out what type of value a variable refers to
    - TS infers if variable declaration and initialization happen in one expression
    - regarding functions, TS will try to infer return values but not arguments
      - always annotate arguments cuz we have to
      - always annotate return values anyways in case we mess up
  - destructuring with annotations
    - replace var with destructuring statement
- only active _during development_
  - TS code is compiled into JS before deployment
  - [typescriptlang.org](https://www.typescriptlang.org/)
- doesn't provide any performance optimization
  - has no effect on how our code gets executed by browser or Node
- `npm install -g typescript ts-node`
  - ts-node is a CLI tool for compiling and executing TS easily from terminal
    - combines commands `tsc <fileName>.ts` to transpile and `node <fileName>.js` to execute
- _tuples_
  - array-like structure where each element represents some property of a record
    - specific ordering of elements inside of it
    - can be created with `type` aliases
  - useful for csv files when representing rows
  - not very useful otherwise as the array values have no context
    - more useful to have key/value objects
- interfaces
  - creates a new type, describing the property names and value types of an object
  - general strategy for reusable code
    - create functions that accept arguments that are typed with interfaces
    - objects/classes can decide to implement a given interface to worok with a function
    - class _implements_ interface
- class method modifiers
  - public
    - can be called anywhere, any time
  - private
    - can be called by other methods in _this_ class
  - protected
    - can be called by other methods in _this_ class, or by other methods in child classes
  - overwriting a method cannot change the modifier in the child class
  - can be used on methods but also properties/fields in the class
    - `constructor(public color: string) {}`
      - will be assigned as an instance var
- abstract classes
  - cannot be instantiated
  - used to set up requirements for subclasses
  - Do create a CLass when translated to JS
    - TS compiles to JS, interfaces don't survive the translation, don't exist in JS world
    - can be used for `instanceof` checks
    - list out all the properties that any extending subclass must have
      - similar to implementing interfaces
        - `abstract statusCode: number`
          - means subclass _must_ implement that abstract member property
- angle brackets => generic syntax
  - functions or types passed as arguments
  - customize the types being used in a function, class or interface
  - `<T>` refers to argument 1
- TS evaluations misc
  - swift guard `?`
    - when checking parameters on potentially null or undefined objects like the session
    - `if (!req.session || !req.session.jwt)` === ` if (!req.session?.jwt)`
  - bang `!`
    - override warnings for potentially undefined objects
- Augmenting Type Definitions
  - `declare global { namespace Express { interface Request { customOptionalParameter?: CustomType }}}`
- [readonly modifier](https://www.typescriptlang.org/docs/handbook/classes.html#readonly-modifier)
  - like `final` in java, makes sure a given property does not get changed
- Typescript getters: get accessors
  - throw an error if someone tries to access a NATS client variable before it's been connected
  - `get client() { ...code }`, access NATS client by using `client()` property
  - when calling in another file, no need to invoke with `()`
  - its not a function we call, its a function that defines the property on the instanec
    - `new SomePublisherClass(natsWrapper.client)`

Express notes

- express route validation with [express validator](https://express-validator.github.io/docs/)
  - can validate incoming request body, parameters like :id's, and query strings
  - use `validationResult` object to store any errors occuring during request validation to be used in response
    - convert to json array object with `.array()`
      - `return res.status(400).send(errors.array())`
  - this package has its own convention about what params are sent back on the response
  - in microservices environment, different services could use different backends with different response structures
    - we must have a consistenly structured response from _all_ servers, no matter what went wrong
      - create a consistent error response structure from any service with a shared library middleware
    - capture all possible errors using Express' [error handling mechanism](http://expressjs.com/en/guide/error-handling.html#error-handling)
      - async handlers need to manually capture error (and pass it off to the `next` function)
        - or use express-async-errors seen below
    - convey info from Request Handler to Middleware through Error object
      - when we `throw` an error, _express_ first looks for handler with 4th var (_err_, req, res, next)
        - this will be used as error middleware
    - with TS we'll have to subclass custom errors
      - set prototype when extending a class in TS thats built into the language, e.g. Error
        - `Object.setPrototypeOf(this, ClassName.prototype);`
  - custom validation for mongoose id in request body
    - `.custom((input: string) => mongoose.Types.ObjectId.isValid(input))`
- [express-async-errors](https://www.npmjs.com/package/express-async-errors)
  - changes the default behavior of how express handles route handlers
  - implements `await` on any `async` function
  - install and require after express

Testing

- types of tests
  - unit test how a single piece of code, e.g. single middleware
  - test how different different pieces of code, components, or services work together
    - testing how services work together can be very complicated, requiring a cost effective test environment in k8s
    - better to test services in isolation
- goals and steps
  - 1: basic request handling
    - assert against an expected auth cookie, db entry, status code returned, etc
    - Jest test runner
      - start in-memory copy of mongodb, mongodb-memory-server
      - start up express app
      - use [supertest](https://www.npmjs.com/package/supertest) library to make fake req's to express app
      - run assertions to make sure request was correctly processed
  - 2: unit testing against models and their functionality
  - 3: emitting and receiving events in service
    - simulates different services working together
- regarding supertesting

  - running tests on multiple services concurrently on the same port will cause errors
    - requires access to dev/prod app env, to make use of [ephemeral](https://www.npmjs.com/package/supertest#example) ports
    - split app setup and instantiation for porting
  - testing against cookies
    - supertest `request` returns the response object
      - use `.get()` to inspect headers of response, like Set-Cookie
    - NB: cookie-session `secure` enforces https, incompatible for testing
      - set secure to false in test environment
      - Jest sets terminal process.env.NODE_ENV variable to `'test'` when running tests
    - when signing out, session is set to empty and cookie expiration is set to past timestamp
      - `'express:sess=; path=/; expires=Thu, 01 Jan 1970 00:00:00 GMT; httponly'`
  - issues with testing cookies
    - browser and postman both have built-in functionalty to manage cookies
      - supertest does not...
        - pull 'Set-Cookie' auth cookie off response and include in followup request
          - `.set('Cookie', cookie)`
  - extracting duplicate auth signin logic to testing setup
    - global async function that executes and returns the cookie
    - adding typescript support for global func
      - `declare global { namespace NodeJS { interface Global { myFunction(): Promise<arrayOfTypeReturned[]>`
    - instantiate cookie in jest it block and set on supertest request headers
    - alternate to simply extracting function to its own file and importing into test file

- how to run tests
  - directly from terminal without docker
  - implies _local env_ is capable of running each service
    - complex projects might make this hard
- test library dependencies
  - `install --save-dev @types/jest @types/supertest jest ts-jest supertest mongodb-memory-server`
  - exclude from dockerfile with `npm install --only=prod`
  - package.js for ts support with jest
    - test script `--no-cache`
    - jest config with ts-jest preset, node testEnvironment, setupFilesAfterEnv startup script
    - sometimes a test fails despite being fixed, ts-jest or jest not detecting changes made to files
      - restart test runner with ctrl-c and npm run test
- environment variables in jest test env
  - set `process.env.whatever` in `beforeAll` block
- regarding NATS: publishers need a nats client wrapper singleton in test env
  - can either set test env to connect to nats server
    - not ideal, can't assume we always have a running nats event bus
  - or fake it out with a mock wrapper in Jest
- Mocking Imports with Jest
  - find the file you wanna fake
  - in the same directory, create a folder called `__mocks__`
  - in that folder, create a file with identical name to target file
  - write fake implementation
  - tell jest to use fake file in test file
  - `jest.fn()` will internally keep track of the number of times it was invoked and the arguments provided
    - \_isMockFunction: true,
    - getMockImplementation: [Function (anonymous)],
    - mock: [Getter/Setter],
    - mockClear: [Function (anonymous)],
    - mockReset: [Function (anonymous)],
    - mockRestore: [Function (anonymous)],
    - mockReturnValueOnce: [Function (anonymous)],
    - mockResolvedValueOnce: [Function (anonymous)],
    - mockRejectedValueOnce: [Function (anonymous)],
    - mockReturnValue: [Function (anonymous)],
    - mockResolvedValue: [Function (anonymous)],
    - mockRejectedValue: [Function (anonymous)],
    - mockImplementationOnce: [Function (anonymous)],
    - mockImplementation: [Function (anonymous)],
    - mockReturnThis: [Function (anonymous)],
    - mockName: [Function (anonymous)],
    - getMockName: [Function (anonymous)]

Code Sharing and Reuse Between Services

- Options for sharing logic
  - 1: copy pasta, boooooo
  - 2: Git Submodule
    - separate repo for common code, added to target repo as submod
      - version control and documentation is good, albeit painful setup
  - 3: NPM package
    - move common code into new project, _publish_ it as a package to NPM registry, install as npm dependency
      - version control, documentation, and ease of use
        - but changing common code has several steps and can be tedious (republish, update package.json)
- NPM Registry
  - publish packages to NPM public registry, to public/private registry _inside an organization_, or to a private registry
    - private organizations/registries cost money
      - alternatively, host your own open source version of npm registry to go free-tier
  - sign up for an account at [npm](npmjs.com)
    - '+ Add Organization'
  - publishing npm modules
    - create directory and run `npm init -y` to create the package.json
    - for `name` field, enter `@organizationName/packageName`
    - inside directory, create git repo
      - `git init`, `git add .`, `git commit -m 'initial commit'`
    - to publish => `npm publish --access public`
      - without `--access public` then npm assumes it to be private within organization
      - might have to run `npm login` with credentials to publish
    - transpile TS into JS to avoid versioning errors
      - install typescript del-cli --save-dev
      - `--tsc init`
      - tell TS where to find source code and where to place it after transpiling to JS
        - add `tsc` command to package.json scripts
        - in tsconfig
          - uncomment `declaration`
            - generates type def file when transpiling for backwards compatability
          - uncomment `outDir` and add target directory
        - run tsc command in packagejson
      - ensure build is fresh, remove everything in build before rebuilding
        - del-cli script in packagejson
          - e.g. `"clean": "del ./build/*"`
      - more edits to packagejson
        - edit `main` to reflect main file being imported
        - add `types` file pointer for ts projects
        - add `files` array to tell npm which files must be included in final published version of package
      - manually save changes, push to git, and increment version number
        - or use `npm version patch` to have npm automatically increment before running `npm publish`
        - semantic versioning
    - create easy importing of submodules/dirs for users of the npm package
      - import then export everything in the index file
        - `export * from './errors/bad-request-error';`
          - automatically imports then immediately exports
          - one line for every file
          - make sure anything imported is actually installed
      - `npm update @package/dir` on CLI inside proper project
  - to verify container updated version of module
    - `kubectl exec -it <pod-name> sh` to open k8s shell on pod

[NATS](https://docs.nats.io/) Streaming Server - Event Bus Implementation

- **NATS** and **NATS Streaming Server** are 2 different things
  - NATS is a simple implementation of event sharing, the more advanced streaming server is built on top of NATS
- [nats streaming docker documentation](https://hub.docker.com/_/nats-streaming)
  - to set custom commandline options to be executed when container starts up, provide them in yaml `containers` section
    - `args` is an array of arguments to provide to primary command that gets executed when container is built from image
- options to connect to NATS pod in the cluster
  - #1: program => **ingress-nginx** <=> NATS ClusterIP Service <=> NATS Pod
  - #2: program => **NodePort Service** <=> NATS Pod
  - #3: program => **Port-Forward Port 4222** => NATS Pod
    - define port-forwarding on cluster with kubectl (strictly in a development setting) from CLI
      - `kubectl port-forward <pod-name> localPortNumber:portOnPod`
    - useful for breaking cluster connection
      - simply stop the process running the port-forward command
- [node-nats-streaming](https://www.npmjs.com/package/node-nats-streaming)
  - client library used to communicate with NATS
  - requires services to subscribe to _channels_ (aka topics (or subjects)) to listen for events
  - stores all events in memory by default
    - can customize to store in flat files on harddrive or in a MySQL or Postgres db
  - documentation refers to `stan` (nats spelled backwards): aka the client
  - cannot make use of async/await, use event-driven approach
    - after client connects to nats server, emits connect event
      - listen for this event with `stan.on('connect', () => {});`
        - takes callback func as second arg, executed after
        - `connect` args: cluster id, client id, then url we want to connect to
    - is NATS world, events are more commonly referred to as _messages_
  - NATS can only share raw data, cannot share plain javascript objects, must convert to JSON
  - `publish` function
    - 1st arg is name of channel (subject)
    - 2nd arg is the data being shared
    - 3rd arg is optional callback func
  - `subscribe` function, receive data through subscription
    - create subscription, listen for particular event off the subscription
    - `subscription.on('message', (msg) => {})`;
      - optional second argument for queue groups, see below
  - `subscriptionOptions`, set additional options by chaining on additional calls
    - eg `setDeliverAllAvailable`, `setManualAckMode`, `setMaxInFlight`
    - provide as third arg to `subscribe`
    - default behavior, if there's an error in the service when receiving the event, it's lost, gone, poof
      - customize with this with setManualAckMode (acknowledge)
      - if we do not acknowledge the event by deadline (30 sec or smt), NATS resends event
        - `msg.ack()`: service acknowledges successful processing to node-nats-streaming-server
- accessing event data with typescript, check out `Message` type definition file to see useful functions on a message
  - `getSubject()` returns the name of the channel that the message came from
  - `getSequence()` returns the number of that event
    - event counting starts at 1, not 0
  - `getData()` returns data in the event/message
- NATS never wants to see a duplicate clientID
  - if we want to scale, run multiple copies of a listener/publisher client, must have different ids
- **queue groups**
  - allows gatekeeping when there's multiple copies of a client (dont want same msg to get processed by both copies of client)
  - created inside of a channel
  - can have multiple q groups associated with one channel
  - subscription joins queue group, second argument to `subscribe` is name of q group
  - when event comes into channel, nats sends event to only one member of the group
  - can still have other services that are not a member of the queue group, but are still listening to the channel
  - must be unique for all the different services that will create subscriptions inside the channel
    - must stay consistent over time
- client health checks
  - to check missing events and logs, port forward monitoring service
  - `kubectl port-forward nats-depl-594c9945f6-2cffp 8222:8222`
    - go to `localhost:8222/streaming`
    - [monitoring](https://docs.nats.io/nats-server/configuration/monitoring)
    - `localhost:8222/streaming/channelsz?subs=1`
  - adjust heartbeat values in k8s deployment
  - override SIGINT and SIGTERM with `stan.close()` to override default waiting behavior
    - NATS will wait for client to restart which can cause delays in event reprocessing
- use name of pod as client id in k8s environment variable
  - when spinning up multiple listeners, allows each to have unique name
  - useful for checking logs of each listener pod
    - eg: `name: NATS_CLIENT_ID valueFrom: fieldRef: fieldPath: metadata.name`
- **core concurrency issues**

  - listener can fail to process the event
  - one listener runs more quickly than another
    - events get processed out of order
  - NATS might think a client is still alive when it is dead
  - we might receive the same event twice
    - NB: these things happen with sync communication and monolith style apps too, not just async
  - solutions that will not work
    - limit to just one service
      - creates bottleneck and can still fail regardless
    - try to handle every error case
      - almost infinite number of ways to fail, too expensive
    - share state between services of last event processed, to process in order
      - sequential processing of events has a big performance penalty
    - last event processed tracked by resource ID, e.g. giving each user their own sequence
      - with NATS, to get a restarted sequence, would require a totally different channel
      - max of 1000 channels by default, too much processing overhead
    - last id processed with publisher db recording last sequence
      - sending event from publisher to NATS is a one way operation, cannot get the last seq
  - real solution
    - record last transaction number/version per resource/transaction
      - use DurableName in conjunction with DeliverAllAvailble and a queue group
        - DeliverAllAvailble: gets all events emitted in the past
        - setDurableName: keep track of all events that have gone to that subscription
        - queue group ensures NATS will not dump the durablename subscription and that events only go to one instance of service
    - recap: versioning records to avoid concurrency issues when emitting events (rapidly)
      - first time record is saved to db, version parameter is initiated to 1
        - any updates to that record increments the version num by 1
      - event is processed: record saved to db, then event published to listening services
      - consuming service (with its own db), looks into db for eventId, then compares version number
      - if event version is not equal to db version + 1, listener will time out because `msg.ack()` is not called
        - nats-streaming-server will reemit event after 5 (customizable) seconds
        - hopefully, the missing event will be processed within that time frame
    - mongoose and mongodb can manage all the version stuff automatically
      - [optimistic concurrency control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control)
      - [mongoose-update-if-current](https://www.npmjs.com/package/mongoose-update-if-current)
        - updates 'version' field of document automatically
        - "Default behaviour is to use the schema's version key (\_\_v by default) to implement concurrency control"
          - `__v` is one of the properties listed in the mongoose.Document interface
          - instead, we'll configure it to use our own _version_ field
            - by adding `version` to doc interface
            - `ticketSchema.set('versionKey', 'version');`
            - `ticketSchema.plugin(updateIfCurrentPlugin);`
      - include id _and_ the version num in request to find and _update_ a record in db
        - when should we increment the 'version' number of a record with an event?
          - whenever the **primary service responsible for a record** emits an event to describe a **create/update/destroy** to a record

- handling publish failures

  - whenever we save record to DB, immediately emit event (publish)...
  - rather than emit, instead save event to events collection DB
    - triggers a publish to NATS, which updates events collection DB flag
  - this 2 stage approach helps in multiple ways
    - if theres an issue connecting to DB, it won't save transaction or event
    - if nats is down, can still save transaction and event, and publish to NATS later
      - solves issue where we save a record and fail to publish
  - however, need to make sure that if transaction is saved but event collection is not, or visa versa, it rolls back transaction
    - wrap them in a 'db transaction', where if one fails they all fail

- alternatives with cross language support, if using multiple languages across services
  - JSON Schema
    - allows you to define JSON structures, different properties/values, and validations
  - Protobuf
    - a way to serialize info, like JSON, but more compact format
  - Apache Avro
    - themed around java, support for other langs as well
